---
layout: default
---

# 嵌入式AI 双周简报 (2018-07-20)

## 业界新闻

- [国内首款RISC-V开发板Perf-V Creative Board正式发售 | PerfXLab澎峰科技](https://shop350413479.taobao.com/)<br/>
简评：国内首款Perf-V Creative Board现已在淘宝公开发售，搜索关键词“perf-v”即可，目前设有35T、50T、100T三种芯片型号可供选择，Perf-V适配扩展板以及周边产品即将上线。（店铺链接：https://shop350413479.taobao.com）
- [小米开源移动端深度学习框架MACE，自主研发，专为IoT设备优化 | 量子位](https://mp.weixin.qq.com/s/J6IjYz4KCIVHJ0QW8zCxfw)<br/>
简评：在2018开源中国开源世界高峰论坛现场，小米人工智能与云平台副总裁崔宝秋对外宣布，将正式开源小米自研的移动端深度学习框架Mobile AI Compute Engine，简称MACE。
- [英特尔收购芯片公司eASIC，加速FPGA，降低CPU依赖 | 新智元](https://mp.weixin.qq.com/s/h79cfwNRLnQDBW76ddMHyw)<br/>
简评：7月13日据外媒消息，英特尔计划收购小型芯片厂商eASIC，具体财务条款未披露。这一交易将有助于英特尔降低对CPU的依赖，实现业务多元化。
- [“全栈”开源的VTA会给AI芯片产业带来什么？ | StarryHeavensAbove](https://mp.weixin.qq.com/s/5HwVRnX2g2ITB0o2JflNGQ)<br/>
简评：TVM团队刚刚公开了开源项目VTA：Versatile Tensor Accelerator[1]。陈天奇博士的知乎文章称其为“VTA: 开源深度学习芯片栈”[2]。结合TVM，这是一个从软件到硬件完全开源的项目，是目前看到最完整的开源尝试。那么，它会对AI芯片产业产生什么样的影响呢？
- [基于开源架构RISC-V的低功耗边缘计算处理器——GAPUINO开发板评测 | 与非网](https://mp.weixin.qq.com/s/TLqzVXz_sc6sVWB4AnxZnw)<br/>
简评：GAPUINO是一款基于Arduino外型打造的开发平台，板载GreenWaves推出的全球首款基于RISC-V的低功耗边缘计算处理器GAP8。
- [IBM100万忆阻器大规模神经网络加速AI | 新智元](https://mp.weixin.qq.com/s/eyzzeYOKdah-9WGUrhbAkg)<br />
简评：目前将深度神经网络和生物神经网络进行匹配的研究正处于瓶颈期。而近期，IBM公司Irem Boybat等人在《Nature Communication》中发表的文章，有望改善此难题：他们设计了多记忆突触结构(multi-memristive synaptic architecture)，能够在不增加功率密度的情况下提高突触的精度，并在一个拥有100多万台相变存储器(PCM)器件的脉冲神经网络(SNN)中对多记忆突触结构进行了实验演示。

## 论文

- [2018/0336] [让CNN跑得更快，腾讯优图提出全局和动态过滤器剪枝](https://www.ijcai.org/proceedings/2018/0336.pdf)<br/>
简评：网络剪枝是一种加速 CNN 的常用方法。厦门大学和腾讯优图的一项研究提出了一种全新的全局和动态过滤器剪枝方法，能够实现更好的剪枝效果且具有更好的适应性。该论文已被将于当地时间 7 月 13-19 日于瑞典斯德哥尔摩举办的 IJCAI-18 接收。
- [1807.05960] [Meta-Learning with Latent Embedding Optimization](https://arxiv.org/abs/1807.05960)<br/>
简评：基于梯度的元学习技术既广泛适用，又能熟练地解决具有挑战性的小镜头学习和快速适应问题。然而，它们具有在极低数据体系中在高维参数空间中操作的实际困难。我们表明，通过学习模型参数的低维潜在生成表示，并利用潜在嵌入优化（LEO）在该空间中执行基于梯度的元学习，可以绕过这些限制，从而有效地将基于梯度的自适应过程与模型参数的基础高维空间。
- [1807.04320] [Automated Vulnerability Detection in Source Code Using Deep Representation Learning](https://arxiv.org/abs/1807.04320)<br/>
简评：利用深度学习来解决软件中存在的漏洞。
- [1807.02787] [Financial Trading as a Game: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/1807.02787)<br/>
简评：从金融市场获得持续利润的自动程序对每一个市场从业者来说都是有利可图的。最近在深度强化学习方面的进展为这种贸易代理的端到端培训提供了一个框架。
- [1807.01438] [Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation](https://arxiv.org/abs/1807.01438)<br/>
简评：在行人检测中，一个关键的问题是检测那些在图像和视频中引入微弱对比度和动态模糊的小物体，在我们看来，这应该部分地诉诸于根深蒂固的注释偏差。在此基础上，我们提出了一种新颖的方法，结合了本体拓扑线定位（TLL）和时间特征聚合，用于检测多尺度行人，这种方法在距离摄像机相对较远的小型行人中尤其有效。
- [1806.11248] [XGBoost: Scalable GPU Accelerated Learning](https://arxiv.org/abs/1806.11248)<br/>
简评：XGBoost GPU算法升级。
- [1708.06519] [Learning Efficient Convolutional Networks through Network Slimming](https://arxiv.org/abs/1708.06519)<br/>
简评：深度卷积神经网络（CNNs）的部署在很大程度上受到了其高计算成本的阻碍。本文提出了一种新颖的CNNs的学习方案，同时降低了模型的尺寸;2）减少运行时内存占用;3）降低计算操作的数量，而不影响准确性。
- [1807.02291] [上海交大搞出SRNN，比普通RNN也就快135倍](https://arxiv.org/abs/1807.02291)<br/>
简评：上海交大的研究人员提出了切片循环神经网络（Sliced recurrent neural networks，SRNN）的结构，在不改变循环单元的情况下，比RNN结构快135倍。
- [1806.09055] [指数级加速架构搜索：CMU提出基于梯度下降的可微架构搜索方法](https://arxiv.org/abs/1806.09055)<br/>
简评：本论文用可微的方式重构架构搜索任务，解决了该任务的可扩展性难题。与在离散和不可微搜索空间中使用进化算法或强化学习的传统方法不同，我们的方法基于架构表征的连续松弛，利用梯度下降实现架构的高效搜索。我们在 CIFAR-10、ImageNet、Penn Treebank 和 WikiText-2 上进行了大量实验，结果表明我们的算法在发现高性能的图像分类卷积架构和语言建模循环架构中表现优异，且该算法的速度比之前最优的不可微方法快了几个数量级。

## 开源项目

- [MACE 设计与实践](https://github.com/XiaoMi/mace)<br/>
简评：MACE重磅开源。
- [Computer Vision Annotation Tool (CVAT) is a web-based tool which helps to annotate video and images for Computer Vision algorithms](https://github.com/opencv/cvat)<br/>
简评：基于web的视频图像标记工具。
- [lagom: A light PyTorch infrastructure to quickly prototype reinforcement learning algorithms.](https://github.com/zuoxingdong/lagom)<br/>
简评：用于强化学习算法快速原型构建的轻量级PyTorch架构。
- [simple neural network library in ANSI C](https://github.com/codeplea/genann)<br/>
简评：ANSI C简单神经网络库。

## 博文
- [An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution](https://eng.uber.com/coordconv/)<br/>
简评：卷积网络的问题及其解决方案CoordConv——CoordConv解决了坐标变换问题，具有更好的泛化能力，训练速度提高150倍，参数比卷积少10-100倍。
- [How fast is my model?](http://machinethink.net/blog/how-fast-is-my-model/)<br/>
简评：模型到底能跑多快？深度学习计算复杂度解析。
- [AI Can Now Fix Your Grainy Photos by Only Looking at Grainy Photos](https://weibo.com/tv/v/Gpnqtnipl?fid=1034:4260482161558120)<br/>
简评：用深度学习自动修复图像颗粒状/像素化噪声和文字水印.
- [如何将模型部署到安卓移动端，这里有一份简单教程 | 机器之心](https://mp.weixin.qq.com/s/23FoaaA3Z_3kf03BmepFPg)<br/>
简评：本文介绍了如何利用 TensorFlow Mobile 将 PyTorch 和 Keras 模型部署到安卓移动端。
- [图像压缩哪家强？请看这份超详细对比 | PaperWeekly](https://mp.weixin.qq.com/s/B7reSwa9sCZqbkYVM5-VOA)<br/>
简评：图像压缩在计算机视觉领域占据着比较重要的位置，随着 GAN，VAE 和超分辨率图像让生成模型得到了很大的进步。不同的模型有着不同的性能优势，本文用精炼的语言加上较为严谨的实验对比了 GAN，CAE 和 super-resolution 在图像压缩性能上的优势。
- [当前训练神经网络最快的方式：AdamW优化算法+超级收敛](http://www.fast.ai/2018/07/02/adam-weight-decay/)<br/>
简评：最优化方法一直是机器学习中非常重要的部分，也是学习过程的核心算法。而 Adam 自 14 年提出以来就受到广泛关注，目前该论文的引用量已经达到了 10047。不过自去年以来，很多研究者发现 Adam 优化算法的收敛性得不到保证，ICLR 2017 的最佳论文也重点关注它的收敛性。在本文中，作者发现大多数深度学习库的 Adam 实现都有一些问题，并在 fastai 库中实现了一种新型 AdamW 算法。根据一些实验，作者表示该算法是目前训练神经网络最快的方式。
----

Editor: 王建章、袁帅

----

<a rel="license" href="http://creativecommons.org/licenses/by-sa/2.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/2.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-sa/2.0/">知识共享署名-相同方式共享 2.0 通用许可协议</a>进行许可。
