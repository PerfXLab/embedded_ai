---
layout: default
---

# 嵌入式AI 双周简报 (2017-11-14)

## 业界新闻

- [寒武纪3款AI处理器齐发，陈天石3年小目标：占领10亿智能AI终端；占领中国AI云端高性能芯片1/3 | 新智元](https://mp.weixin.qq.com/s?timestamp=1510651761&src=3&ver=1&signature=VVbCCFGdhnib9UHziJcEvXH1oU9L2jQ4SJQBcYVKilDmi*NbqEut3Xuk*1Dh5sKzSsbzNP0zTtqjq70w0ory-GQ83TjfaC03Fdw7ajfYhra6Gi1DXImuzfjnpIpxoVcT-FnAHklFtEMUIdy-y5GAVThf3*vsT6-g6PoleTGBm2o=) <br />
简评：AI芯片独角兽寒武纪成立以来首次发布会召开，联合创始人兼CEO陈天石发布了系列新品及公司未来路线图——“3年内占领10亿智能AI终端，占领中国云端高性能芯片1/3市场份额”。至于如何与英伟达竞争，陈天石告诉新智元：“提供性能功耗比更好的芯片。这个市场很大，其实未见得是零和博弈。”。
- [两项业界最佳的普林斯顿新算法自动生成高性能神经网络，同时有超高效压缩 | 新智元](https://mp.weixin.qq.com/s?timestamp=1510651739&src=3&ver=1&signature=VVbCCFGdhnib9UHziJcEvRoYvJGiaDTMYilDhkqlT5k7DFJ0k9W8DKmYYk5SvqA0it7XFeHsKlXJ3f0xAdQuXFNEqeYw8DJ7yuPiLFk6IYKLquvpL16a80ms*L27lptGJTyHaAPWcL4-rEg2svxkSZO*w6YCQZ*-TpCQbMv0thE=) <br />
简评：普林斯顿大学研究人员提出了一种会在训练过程中连接、生长、移除神经元的神经网络。这种神经网络使用梯度和神经元强弱来生长（grow）和修剪（prune），从而实现权重和结构的同时训练。此算法可同时实现神经网络结构的自动选择和超高效压缩。所取得的压缩率，所获得的神经网络模型均为当前业内最好纪录。
- [阅面科技发布堪比服务器的终端视觉模块，将云端计算能力搬至终端 | 机器之心](https://mp.weixin.qq.com/s?timestamp=1510652144&src=3&ver=1&signature=VVbCCFGdhnib9UHziJcEvRPXlaG0XB7H9TSQbqZDna31FbbWi1rNDSafBnfMt85qqFTkmaq8WoKW6OKzfaFFiW7MrslH9BdQsj*3PKJ1gKBAdpqSfnuhSyXW7Iwxri1sx1ZJ52LVmDyBLeqyHA8zx03bDc1Z4bCV7LoL52T6KgA=) <br />
简评：11 月 1 日，阅面科技在深圳举办了创立两年来的首次新品发布会，共发布了三款产品：跨模态人脸识别引擎 UniFace、基于 Uniface 的「繁星」AI 芯片视觉模块、以及基于「繁星」的智能客群分析摄像机——「阅客」。
- [如何看待博通拟以 1300 亿美元收购高通？ | 知乎](https://www.zhihu.com/question/67615840/answer/255421766) <br />
简评：最后高通董事会一致拒绝了博通提出的收购提议，并称该提议不符合股东最佳利益，且严重低估了高通的价值。有消息称，博通或在考虑继续提高对高通报价的可能性，包括通过增加债券融资等手段，但目前尚不清楚博通将会在何时跟进。


## 论文

- [A Berkeley View of Systems Challenges for AI](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-159.pdf) <br />
简评：这个survey主要介绍了下一代的人工智能系统（包含系统、架构、安全性等），整体较短。主要关注点在未知环境中做出安全可信的决策，个性化智能以及在日益增长的数据这一挑战下的应对方法。相信可以给在做嵌入式人工智能的同学们一些启发。
- [1711.01243] [ResBinNet: Residual Binary Neural Network](https://arxiv.org/abs/1711.01243) <br />
简评：近来二值网络提供了内存和执行效率上的优势。本文提出的ResBinNet通过将residual binarization和temperature adjustment的策略结合，前者是可以在一层上学到多级别的binary特征表达，后者可以逐渐对特定层的权重进行binarize。两种策略结合得到一系列的soft-binarized参数，提高原始binary net的收敛率和精度。而且该方法可以在infer时间和准确率之间权衡。
- [1711.02613] [Moonshine: Distilling with Cheap Convolutions](https://arxiv.org/abs/1711.02613) <br />
简评：大家知道distillation策略通过小的student net来学习teacher net，小网络相比teacher网络结构和层数等更精简，方便部署在资源有限设备上。本文在原本student-teacher的基础上将，通过attention tansfer的策略让student网络相比直接在原始数据上训练有了更好的性能。
- [1711.03386] [Performance Evaluation of Deep Learning Tools in Docker Containers](https://arxiv.org/abs/1711.03386) <br />
简评：Docker的使用已经非常广泛，甚至有一些小伙伴在嵌入式上部署Docker来简化部署环境。先不考虑内存占用量，单就在实际使用中在磁盘IO、CPU和GPU上的性能损失又是怎么样的呢？本文将对这些角度进行深入的分析。
- [1711.03016] [DLVM: A modern compiler infrastructure for deep learning systems](https://arxiv.org/abs/1711.03016) <br />
简评：作者借鉴LLVM的思想提出DLVM，认为现在深度学习环境性能和可靠性不能兼顾。已有的深度学习框架在算法描述上使用Python不安全的DSL。DLVM作为一个基础组件，可以自动生成GPU代码，并作domain-specific的优化。相比已有的深度学习编译器IRs来说，DLVM更具有模块化和通用性。
- [1711.02213] [Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks](https://arxiv.org/abs/1711.02213) <br />
简评：
- [1711.00705] [Efficient Training of Convolutional Neural Nets on Large Distributed Systems](https://arxiv.org/abs/1711.00705) <br />
简评：


## 开源项目

- [DPED Project](http://people.ee.ethz.ch/~ihnatova/) <br />
简评：基于深度卷积网络的图片自动美化（DSLR）。
- [KarenUllrich/Tutorial_BayesianCompressionForDL: A tutorial on "Bayesian Compression for Deep Learning" published at NIPS (2017).](https://github.com//KarenUllrich/Tutorial_BayesianCompressionForDL) [[paper](https://arxiv.org/abs/1705.08665)] <br />
简评：采用贝叶斯神经网络压缩的方法。结合最小描述长度原则和变分推断的思想，能够达到700倍的模型体积压缩和50倍的网络加速。
- [emedvedev/attention-ocr: A Tensorflow model for text recognition (CNN + seq2seq with visual attention) available as a Python package and compatible with Google Cloud ML Engine.](https://github.com//emedvedev/attention-ocr) <br />
简评：用于文本识别的Tensorflow的CNN+seq2seq模型。
- [zeusees/HyperLPR: 基于深度学习高性能中文车牌识别 High Performance Chinese License Plate Recognition Framework.](https://github.com//zeusees/HyperLPR) <br />
简评：基于深度学习高性能中文车牌识别。
- [zhixuhao/unet: unet for image segmentation](https://github.com/zhixuhao/unet) <br />
简评：(Keras)Unet图像分割。
- [a-jahani/Real-time-Video-Mosaic: An implemetation of automatic panorama using opencv in C++](https://github.com//a-jahani/Real-time-Video-Mosaic) [[paper](http://ieeexplore.ieee.org/document/7886813/)] [[video](https://weibo.com/tv/v/FuGU2rksQ?fid=1034:51c7bf9fcfeb6d58f4ec80ebb9075712)]<br />
简评：(C++/OpenCV)实时视频全景拼接。
- [experiencor/basic-yolo-keras: Implementation of YOLO version 2 in Keras](https://github.com//experiencor/basic-yolo-keras) [[blog](https://experiencor.github.io/yolo_keras.html)] <br />
简评：用Keras实现YOLOv2。
- [Carla – Open source simulator for autonomous driving](http://carla.org/) [[code](https://github.com/carla-simulator/carla)] [[video](https://weibo.com/tv/v/FuUhUEL31?fid=1034:9fd5399bbb60e1a7e6ed6e7537bbfb55)] <br />
简评：CARLA：开源自驾车模拟环境。
- [Neural Network Designer](https://itunes.apple.com/cn/app/id1294441403) [[blog](https://www.objectsandsuch.com/neural-network-designer)]<br />
简评：Neural Network Designer是mac上用来可视化编辑设计基于人工神经元网络的工具，可用于测试各种网络的设计和执行。



## 博文

- [深鉴科技姚颂：深度学习处理架构的演进 | 雷锋网](https://www.leiphone.com/news/201710/9EFp9GekEnMcMHyg.html) <br />
简评：深度学习、体系结构、数据规模的共同发展促进了人工智能行业的繁荣。在通用架构之外，深度学习处理架构已经经历了三代的发展，从计算优化、存储优化，到结合Deep Compression的稀疏化处理架构。深鉴科技姚颂为大家讲解深度学习处理架构的演进过程，以及几个核心问题是如何逐渐解决的。
- [Copista: Training models for TensorFlow Mobile | Medium](https://medium.com/@tinyline/copista-training-models-for-tensorflow-mobile-2cf4cb1674e4) <br />
简评：Copista：TensorFlow手机端(画风迁移)模型训练。
- [TensorFlow下构建高性能神经网络模型的最佳实践 | 人工智能头条](https://mp.weixin.qq.com/s/9iBjwM3EYleMUlAzkKlVJg) <br />
简评：文中简述了常见的网络压缩原理，并用TensorFlow的模型压缩工具量化网络对结果进行了简单的分析。
- [Seamless Google Street View Panoramas | Google Blog](https://research.googleblog.com/2017/11/seamless-google-street-view-panoramas.html) [[video](https://weibo.com/tv/v/FunJ3rPCv?fid=1034:b094772598613db5b1e40cbc4f3bbfe9)] <br />
简评：Google基于光流新算法实现街景全景无缝拼接。
- [从公司商用层面，未来哪些SLAM方案能成为趋势?以及难点突破点是什么？ | 知乎](https://www.zhihu.com/question/53232703) <br />
简评：根据场景不同，SLAM的方案从传感器到算法千差万别。一定要根据某一应用场景的核心诉求来考虑方案。
- [MobileNet在手机端上的速度评测：iPhone 8 Plus竟不如iPhone 7 Plus | 机器之心](https://mp.weixin.qq.com/s/mcK8M6pnHiZZRAkYVdaYGQ) [[英文原文](https://medium.com/vitalify-asia/real-time-deep-learning-in-mobile-application-25cf601a8976)] <br />
简评：今年4 月谷歌发布 MobileNets：一个可在计算资源有限的环境中使用的轻量级神经网络。6 月苹果推出 Core ML，允许机器学习模型在移动设备上运行。配备高端GPU的iphone比Mac Book Pro上跑得还要快。本文将介绍实际应用情况和效率。
- [芯片巨头三国杀：AI加剧芯片厂商间竞赛，英特尔、英伟达、AMD竞相发力 | 新智元](https://mp.weixin.qq.com/s/nAHJENflvt_o1-LAyDicFw) [[英文原文](https://www.wsj.com/articles/artificial-intelligence-is-fueling-an-arms-race-among-chip-makers-1510228801)] <br />
简评：AI加剧芯片厂商间竞赛，竞争达到白热化。AI 软硬件市场每年的增长率达50%，英伟达、英特尔和AMD都在这一方向上发力角逐。投资者亦看好这一领域。



----

Editor: 张先轶、袁帅

----

<a rel="license" href="http://creativecommons.org/licenses/by-sa/2.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/2.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-sa/2.0/">知识共享署名-相同方式共享 2.0 通用许可协议</a>进行许可。
